{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # use GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcdc68",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f949c11",
   "metadata": {},
   "source": [
    "First let's implement the patchification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ea732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, patch_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) * (img_size // patch_size) \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels = 3, \n",
    "                              out_channels = embed_dim, \n",
    "                              kernel_size = patch_size, \n",
    "                              stride = patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x)   # (batch_size, dim, img_size // patch_size, img_size // patch_size)\n",
    "        # the input to the transformer should be of shape (batch_size, num_patches, embedding dim)\n",
    "         # (batch_size, dim, img_size // patch_size, img_size // patch_size) --> flattten --> (batch_size, dim, num_patches) --> tranpose --> (batch_size, num_patches, dim)\n",
    "        x = x.flatten(2).transpose(1,2)    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b2868",
   "metadata": {},
   "source": [
    "Now let's define the Feed Forward Layer of the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5bfa3",
   "metadata": {},
   "source": [
    "Now let's define the Multi-Head Attention of the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f422f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, heads = 8, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.keys = nn.Linear(dim, dim)\n",
    "        self.values = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # N is the total number of patches\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        x = self.norm(x)         # (B, N, C)\n",
    "        \n",
    "        query = self.query(x)    # (B, N, C)\n",
    "        key = self.keys(x)       # (B, N, C)\n",
    "        value = self.values(x)   # (B, N, C)\n",
    "        \n",
    "        dim_head = C // self.heads\n",
    "        \n",
    "        # Split (B,N,C) into (B, N, num_heads, dim_head) and permute heads which yields a shape of (B, num_heads, N, dim_head)\n",
    "        # each of the heads, should have (N, dim_head)\n",
    "        query = query.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)   \n",
    "        key = key.reshape(B, N, self.heads, dim_head).permute(0,2,1,3)   \n",
    "        value = value.reshape(B, N, self.heads, dim_head).permute(0,2,1,3) \n",
    "    \n",
    "        # (B, num_heads, N, dim_head) with (B, num_heads, N, dim_head) --> (B, num_heads, N, N)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))   \n",
    "         \n",
    "        scale = dim_head ** -0.5   # (1 / sqrt(dim_head))\n",
    "        attention_scores = attention_scores * scale\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) # (B, num_heads, N, N)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        \n",
    "        # extract the values\n",
    "        # (B, num_heads, N, N) matmul (B, num_heads, N, dim_head) --> (B, num_heads, N, dim_head)\n",
    "        out = torch.matmul(attention_scores, value)   \n",
    "        \n",
    "        # (B, num_heads, N, dim_head) --> (B, N, num_heads, dim_head) --> (B, N, C) \n",
    "        out = out.permute(0,2,1,3).flatten(2)   # or we can use .reshape(B, N, -1) rather than .flatten(2)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c0df5",
   "metadata": {},
   "source": [
    "Now let's define the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_layers, heads, dropout = 0.):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                MultiHeadAttention(dim, heads = heads, dropout = dropout),\n",
    "                FeedForward(dim, dropout = dropout)]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b736f01",
   "metadata": {},
   "source": [
    "Now let's combine all the modules we defined above, into our final ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, num_layers, heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patchify = Patchify(image_size, patch_size, dim)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, self.patchify.num_patches, dim))\n",
    "        self.transformer = Transformer(dim, num_layers, heads, dropout = dropout)\n",
    "        self.classifier = nn.Linear(dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patchify(x)         # (batch_size, N, dim)\n",
    "        x = x + self.pos_encoding    # (batch_size, N, dim)\n",
    "        x = self.transformer(x)      # (batch_size, N, dim)\n",
    "        x = x.mean(1)                # (batch_size, dim)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f9106",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad8265",
   "metadata": {},
   "source": [
    "1. Data Transforms\n",
    "\n",
    "Use [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html) as a reference. Apply data transformations to the training images such that you\n",
    "- Crop a random portion of image and resize it to a size of 32x32\n",
    "- Apply random horizantal flipping as a data augmentation strategy. You can check other data augmentations [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py)\n",
    "- Transform it to a PyTorch Tensor with scaled values between 0-1\n",
    "- Standardize it such that it has a mean of zero and standard deviation of 1\n",
    "\n",
    "For the test images, only perform the last two steps\n",
    "\n",
    "Note: \n",
    "- The mean of the CIFAR dataset for each of the RGB Channels is: `(0.4914, 0.4822, 0.4465)`\n",
    "- The standard deviation of the CIFAR dataset for each of the RGB channels is: `(0.2023, 0.1994, 0.2010)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f533b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.RandomResizedCrop(32),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0c6d3",
   "metadata": {},
   "source": [
    "We will work with the CIFAR10 dataset. It is a dataset of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "\n",
    "2. Load (and download if you havent already done so) the training and testing datasets using [torchvision.datasets](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html). Specify the transforms that you have already defined in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c1b28",
   "metadata": {},
   "source": [
    "3. Define your data loader. Use a batch size of 128 for training and 100 for testing. Make sure you shuffle the data loading process in the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76472807",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7589a",
   "metadata": {},
   "source": [
    "Let's visualize some of the images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(inp):\n",
    "    \"\"\"Display image from a PyTorch Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the labels for the first 4 loaded images \n",
    "print(labels[:4])\n",
    "# if we would like to map them to human-readble text labels\n",
    "print(', '.join([classes[labels[b]] for b in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f24fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_show = 4\n",
    "fig, axs = plt.subplots(1, num_images_to_show, figsize=(10,10))\n",
    "np.vectorize(lambda ax:ax.axis('off'))(axs)\n",
    "\n",
    "for j in range(num_images_to_show):\n",
    "    axs[j].imshow(imshow(images[j]))\n",
    "    axs[j].set_title(classes[labels[j].item()])\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.1, hspace = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd60cf5",
   "metadata": {},
   "source": [
    "#### Part 2: Defining the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f3f87",
   "metadata": {},
   "source": [
    "Create an instance of your model and run it with a random tensor to ensure there is no error. Also check that the output shape is (N, 10) where N is your batch size. **Note**: If you are using a GPU, make sure to move both your model AND input tensor to the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(image_size = 32, \n",
    "            patch_size = 4, \n",
    "            num_classes = 10, \n",
    "            dim = 512, \n",
    "            num_layers = 3, \n",
    "            heads = 8, \n",
    "            dropout = 0.1).to(device)\n",
    "\n",
    "random_input = torch.randn(1,3,32,32).to(device)\n",
    "print(model(random_input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48377b41",
   "metadata": {},
   "source": [
    "Refer to the [torch.optim](https://pytorch.org/docs/stable/optim.html) library. Define the optimizer and learning rate. Use SGD with momentum, with an initial learning rate of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae6771",
   "metadata": {},
   "source": [
    "Define the learning rate scheduler (use [cosine annealing](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)) and plot the learning rate variation for 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "lrs = []\n",
    "for _ in range(epochs):\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(lrs)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10cd0c",
   "metadata": {},
   "source": [
    "now define the loss funtion. We will use the [cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss (also see the input shapes that this loss expects to know how to call it later). Note that this loss applied Softmax on the class logits, and therefore you do not need to define the softmax. This loss is equivalent to applying [LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) on an input, followed by [NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fd4ee",
   "metadata": {},
   "source": [
    "Write the training loop funtion for one iteration.\n",
    "\n",
    "- Set the model to training mode (since we use dropout which has a different behaviour in training and testing)\n",
    "- Iterarate through your data loader. Zero-out accumulated gradients in the computation graph, run the inputs to your model and get the outputs, calculate the loss, backpropogate the loss to calculate gradients, and then update the weights\n",
    "- Your training funtion should return the averaged loss and accuracy at the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 200\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_loss = train_loss/(batch_idx+1)\n",
    "        epoch_acc = 100.*correct/total\n",
    "        \n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Epoch {}/{}, Iter {}/{}, Train Loss: {:.3f}, Train Accuracy: {:.3f}'.format(epoch, epochs, batch_idx, len(trainloader),\n",
    "                                                                                   epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a7c5a",
   "metadata": {},
   "source": [
    "Write the testing funtion to get the test accuracy and test loss (minimal modifications to the training funtion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(testloader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    loss = test_loss/(batch_idx+1)\n",
    "    print('Test Accuracy: {:.3f}, Test Loss: {:.3f}'.format(acc, loss))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4d9e9",
   "metadata": {},
   "source": [
    "Loop over all the epochs:\n",
    "\n",
    "- Run the training funtion\n",
    "- Save the losses and accuracies returned at each epoch\n",
    "- Run the test funtion\n",
    "- Save the model/Overwrite the previously saved one if the accuracy improved from the last epoch. Refer [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "\n",
    "For reference, this model should achieve a **test accuracy** of around **59%** after training for 20 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_acc = train(epoch)\n",
    "    losses.append(epoch_loss)\n",
    "    accuracies.append(epoch_acc)\n",
    "    scheduler.step()\n",
    "    acc = test()\n",
    "    state = {'model': model.state_dict(),\n",
    "             'acc': acc,\n",
    "             'epoch': epoch}\n",
    "    if acc > best_acc: \n",
    "        torch.save(state, 'model.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af34911",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('train loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f90ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('train accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b45094",
   "metadata": {},
   "source": [
    "### Let's visualize some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed87763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_images, gt_labels = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4310f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_image = 4\n",
    "input_test_img = test_images[index_test_image].unsqueeze(0).to(device)\n",
    "outputs = model(input_test_img)\n",
    "_, predicted = outputs.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imshow(test_images[index_test_image]))\n",
    "plt.title('Predicted: {}, Ground Truth: {}'.format(classes[predicted.item()],classes[gt_labels[index_test_image].item()]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78862f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
