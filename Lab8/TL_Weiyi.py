# -*- coding: utf-8 -*-
"""TL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s5rJRc2lBmBHCcZxidwKVXHXyuDnR24x

# Transfer Learning

Transfer learning allows us to deal with scenarios where we do not have enough annotated data. It levarages the knowledge gained in solving a related task to solve the problem of interest.


In this notebook, we will train a classifier on the dogs vs. cats image dataset, but rather than training a Deep Neural Network model from scratch, we'll use a pre-trained Residual Neural Network (ResNet) on the ImageNet dataset as a base. Essentially, this will transfer the knowledge accumulated during the training on a large image dataset with 1000 object classes to a similar problem..
"""

# !wget https://vub-my.sharepoint.com/:u:/g/personal/giannis_bekoulis_vub_be/ETgE24X27zVOot2M_3jo6aEBai14p8gPNbTGmuUVCct2EQ?download=1
# !mv ETgE24X27zVOot2M_3jo6aEBai14p8gPNbTGmuUVCct2EQ?download=1 data.zip
!unzip data_TL.zip

"""## Import Libraries"""

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy


plt.ion()   # interactive mode

"""## Initialize hyperparameters"""

batch_size = 32
num_epochs = 20
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""## Step 1: Load the dataset

We will use the dogs vs cats image dataset as after target data in this exercise. We will load the dataset from the local directory and then use the Pytorch <code>dataset.ImageFolder</code> API to preprocess and passed the images to the <code>torch.utils.data.DataLoader</code>. The dataloader API can load multiple samples in parallel using <code>torch.multiprocessing</code> workers.
"""

transforms = transforms.Compose([transforms.RandomResizedCrop(224),
                                 transforms.RandomHorizontalFlip(),
                                 transforms.ToTensor(),
                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

data_dir = 'data_TL/dogcat'
train_set = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms)
val_set = datasets.ImageFolder(os.path.join(data_dir, 'val'), transforms)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)

train_size = len(train_set)
val_size =  len(val_set)

classes = train_set.classes

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
images, labels = next(iter(train_loader))

imshow(torchvision.utils.make_grid(images))
print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))

"""## Step 2: Model training

In this step, we will load a pretrained model from the Pytorch <code>models</code> library and use it as a base model for our classifier. There are two main approaches to transfer learning:

- Finetuning: In this approach, the neural network is initialized using the weights of a network that has been trained on a large dataset. The network parameters are then further fine tuned for the given task.
- Feature extraction: In this approach, the network parameters for the whole network except the last few fully connected layers are frozen. The last few layers are then trained for the given task

### Define the training loop for
"""

# Commented out IPython magic to ensure Python compatibility.
def train_model(model, criterion, optimizer, num_epochs=25):
    since = time.time()

    model_weights = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    print("Starting Training Loop...")
    for epoch in range(num_epochs):
        # Each epoch has a training and validation phase where we apply model.train() and model.eval()

        # training phase!!!
        running_loss = 0.0
        running_accuracy = 0
        model.train()
        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            # zero the parameter gradients
            optimizer.zero_grad()
            ####
            # forward pass
            outputs = model(inputs)
            #######
            # compute loss
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            ########
            #compute backward pass
            loss.backward()
            #######
            # apply optimizer step
            optimizer.step()
            ######
            # compute loss and accuracy
            running_loss += loss.item() * inputs.size(0)
            running_accuracy += torch.sum(preds == labels.data)
        train_loss = running_loss / train_size
        train_acc = running_accuracy.double() / train_size

        # validation phase!!
        running_loss = 0.0
        running_accuracy = 0
        model.eval()
        with torch.no_grad():
            for inputs, labels in val_loader:
                # >> Your code goes here <<
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)
                running_accuracy += torch.sum(preds == labels.data)
            val_loss = running_loss / len(val_loader.dataset)
            val_acc = running_accuracy.double() / len(val_loader.dataset)


            if val_acc > best_acc:
                best_acc = val_acc
                model_weights = copy.deepcopy(model.state_dict())

        print('[%d/%d]\ttrain loss: %.4f\ttrain accuracy: %.4f\t -- val loss: %.4f\tval accuracy: %.4f'
#                   % (epoch, num_epochs, train_loss, train_acc, val_loss, val_acc))


    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(model_weights)
    return model

"""### Step 2a: Transfer Learning via Fine Tuning

"""

model_ft = models.resnet18(weights='IMAGENET1K_V1')
# reshape the last layer of the ResNet for the binary classification task
# for param in models.parameters():
#     param.requires_grad = False
num_ftrs = model_ft.fc.in_features
# >> Your code goes here <<
model_ft.fc = nn.Linear(num_ftrs, 2)

model_ft = model_ft.to(device)
# Define criterion (cross entropy loss) and optimizer (eg Adam)
criterion = nn.CrossEntropyLoss()
# Observe that all parameters should be optimized now
optimizer =optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

model_ft = train_model(model_ft, criterion, optimizer, num_epochs=25)

"""### Step 2b: Transfer Learning via feature extraction"""

model_fe = torchvision.models.resnet18(pretrained=True)
# freeze the gradients for the network parameters
for param in model_fe.parameters():
  param.requires_grad = False
    # >> Your code goes here <<

    # Parameters of newly constructed modules have requires_grad=True by default
# Define last layer (Linear layer that maps from the features size to the output classes)
num_features = model_fe.fc.in_features
model_fe.fc = nn.Linear(num_features, 2)


model_fe = model_fe.to(device)
criterion = nn.CrossEntropyLoss()

# Observe that only parameters of final layer should be optimized, lr=0.0002, betas=(0.5, 0.999)
optimizer = optim.Adam(model_fe.fc.parameters(), lr=0.0002, betas=(0.5, 0.999))

model_fe = train_model(model_fe, criterion, optimizer, num_epochs=25)

"""## Step 3: Embedding Similarity"""



# graph a random group of 10 samples from cats and another group of 10 random samples from the "dog" class -> get their embeddings
cats_embeddings =
dog_embeddings =


#compute the average cosine similarity and the l2 norm among all the "cats" embeddings - compute every cat-cat pair, then average:
cats_cosine_sim = torch.nn.functional.cosine_similarity(cats_embeddings, cats_embeddings)
cats_l2 = torch.norm(cats_embeddings, dim=1)

#compute the average cosine similarity and the l2 norm among all the "dogs" embeddings, compute every dog-dog pair, then average:
dogs_cosine_sim = torch.nn.functional.cosine_similarity(dog_embeddings, dog_embeddings)
dogs_l2 = torch.norm(dog_embeddings, dim=1)

#compute the cosine similarity and the l2 norm among pairs of cats and dogs embeddings ->  compute every dog-cat pair, then average:
catsvsdogs_cosine_sim = torch.nn.functional.cosine_similarity(cats_embeddings, dog_embeddings)
catsvsdogs_l2 = nn.founctional.pairwise_distance(cats_embeddings, dog_embeddings)

# ANALYSE your results and comment here your observations

"""## Step 4: Feature Visualization"""

model = torchvision.models.resnet18(weights='IMAGENET1K_V1')
# print(model)

model_weights = []
conv_layers = []
model_children = list(model.children())
counter = 0
# append all the conv layers and their respective weights to the list
for i in range(len(model_children)):
    if type(model_children[i]) == nn.Conv2d:
        counter += 1
        model_weights.append(model_children[i].weight)
        conv_layers.append(model_children[i])
    elif type(model_children[i]) == nn.Sequential:
        for j in range(len(model_children[i])):
            for child in model_children[i][j].children():
                if type(child) == nn.Conv2d:
                    counter += 1
                    model_weights.append(child.weight)
                    conv_layers.append(child)
print(f"Total convolutional layers: {counter}")


# ANALYSE your results and comment here your observationss

plt.figure(figsize=(20, 20))
kernels = model_weights[0].clone()
kernels = kernels - kernels.min()
kernels = kernels / kernels.max()
filters = torchvision.utils.make_grid(kernels.detach(), nrow=16, padding=1)
plt.imshow(filters.permute(1, 2, 0))

images, _ = next(iter(train_loader))
test_image = images[0,:,:,:].unsqueeze(0)
imshow(images[0,:,:,:])
results = [conv_layers[0](test_image)]
for i in range(1, len(conv_layers)):
    results.append(conv_layers[i](results[-1]))
output = results[5].permute(1,0,2,3).detach()

plt.figure(figsize=(20, 20))
activation = torchvision.utils.make_grid(output, nrow=16, padding=1, normalize = True)
plt.imshow(activation.permute(1,2,0))