{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/esther/Documents/OneDrive - Department ETRO/PhD/TA/DL_2122/exercises/week_1'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 1: Load dataset, split into training and test sets, and scale features\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load boston housing price dataset\n",
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "print(y.mean())\n",
    "# split into training and test sets, namely 80 percent of examples goes for the training, 20 percent goes for the test set\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_test = x[N_train:,:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "# scale features by removing mean and dividing by the standard deviation\n",
    "x_bar = np.mean(x_train,axis=0)\n",
    "x_std = np.std(x_train,axis=0)\n",
    "x_train_scaled = (x_train - x_bar)/x_std\n",
    "x_test_scaled = (x_test - x_bar)/x_std\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test_scaled.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/esther/Documents/OneDrive - Department ETRO/PhD/TA/DL_2122/exercises/week_1'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 2: Add intercept terms and initialize parameters\n",
    "# Note: If you run this step again, please run from step 1 because notebook keeps the value from the previous run\n",
    "intercept_train = np.ones((N_train,1))\n",
    "x_train_scaled = np.hstack((intercept_train,x_train_scaled))\n",
    "\n",
    "intercept_test = np.ones((x.shape[0] - N_train,1))\n",
    "x_test_scaled = np.hstack((intercept_test,x_test_scaled))\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "\n",
    "# init parameters using random values\n",
    "theta = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/esther/Documents/OneDrive - Department ETRO/PhD/TA/DL_2122/exercises/week_1'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 3: Implement the gradient and the cost function\n",
    "# In this step, you have to calculate the gradient. You can use the provided formula but the best way is to vectorize\n",
    "# that formula for efficiency\n",
    "def compute_gradient(x,y,theta):\n",
    "    n = len(x)\n",
    "    return (1.0/n)*np.dot(x.T,(np.dot(x,theta) - y))\n",
    "\n",
    "def compute_cost(x,y,theta):\n",
    "    n = len(x)\n",
    "    return (0.5/n)*np.sum((np.dot(x,theta) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/esther/Documents/OneDrive - Department ETRO/PhD/TA/DL_2122/exercises/week_1'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 4: Try gradient descent algorithm with different learning rates\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# try different values for the learning rate\n",
    "learning_rates = [0.001,0.003,0.01,0.03,0.1,0.3]\n",
    "\n",
    "# this matrix keeps the learned parameters\n",
    "theta_matrix = np.zeros((len(learning_rates),x_train_scaled.shape[1]))\n",
    "\n",
    "# number of training iterations\n",
    "N_iterations = 100\n",
    "\n",
    "# prepare to plot\n",
    "plt.subplot(111)\n",
    "\n",
    "# calculate cost value and update theta\n",
    "for indx,alpha in enumerate(learning_rates):\n",
    "    # keep the cost value for each training step\n",
    "    J = np.zeros(N_iterations)\n",
    "    \n",
    "    # initialize new parameters using random distribution\n",
    "    theta = 0.5 * np.random.randn(x_train_scaled.shape[1])\n",
    "    for step in range(N_iterations):\n",
    "        # update theta\n",
    "        theta = theta - alpha * compute_gradient(x_train_scaled,y_train,theta)\n",
    "        \n",
    "        # calculate the cost on traing set\n",
    "        J[step] = compute_cost(x_train_scaled,y_train,theta)\n",
    "        \n",
    "    # save the value of theta\n",
    "    theta_matrix[indx,:] = theta\n",
    "    # plot cost function\n",
    "    plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(('0.001','0.003','0.01','0.03','0.1','0.3'), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/esther/Documents/OneDrive - Department ETRO/PhD/TA/DL_2122/exercises/week_1'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 5: Predict the price of house\n",
    "theta = theta_matrix[4,:]\n",
    "predict_price = np.dot(x_test_scaled,theta)\n",
    "\n",
    "# calculate the cost for the test set\n",
    "test_cost = compute_cost(x_test_scaled,y_test,theta)\n",
    "print('test cost: ',test_cost)\n",
    "\n",
    "# plot the ground truth and the predicted\n",
    "x_axis = np.linspace(1,len(y_test),len(y_test))\n",
    "plt.plot(x_axis,y_test,'b',x_axis,predict_price,'r')\n",
    "plt.legend(('Ground truth','Predicted'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Migration to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(x)\n",
    "targets = torch.from_numpy(y)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "#create dataset and dataloader\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "#define regression model\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "# Define optimizer\n",
    "opt = torch.optim.SGD([w,b], lr=1e-5)\n",
    "\n",
    "# Generate predictions without training\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "#compare with targets -> preds should be horrible, compute accuracy to check\n",
    "print(targets)\n",
    "\n",
    "#train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = loss_fn(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "#check now if the loss is lower\n",
    "preds = model(inputs)\n",
    "loss = loss_fn(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "#train the model for 100 epochs\n",
    "fit(100, model, loss_fn, opt)\n",
    "preds = model(inputs)\n",
    "print(accuracy(preds, targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
